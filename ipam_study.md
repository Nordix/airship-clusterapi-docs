# IPAM study

This study aims at proposing a solution for cluster deployments using Cluster
API and Metal3 in environments where dynamic IP address management (DHCP) cannot
be used. It is divided in two phases, one solution on the client side, that is
the most straightforward to implement. The second phase is to implement it and
bake it into a controller logic.

## Scope

### Phase 1

No machinedeployment or controlplane CRs are used. All objects are defined by
the client. The user gives a general configuration for the different objects,
and the client generates the CRs for all machines, baremetalmachines and
kubeadmconfig objects, including the network configuration.

### Phase 2

This is for machinedeployment and controlplane use cases. The logic should be
shifted from the client to a controller to allow the use of templating.

## Restrictions

**All nodes within the same logical unit (nodepool or controller nodes) must have
homogeneous physical networking and identical networking configuration.

In case of upgrade, all logical units (nodepool or controller nodes) must have
enough combined physical resources matching the networking to run the workload
while any one of the nodes is unavailable. For example, if nodepool 1 has three
nodes of 8GB of RAM, it can only run a workload that would fit in 16GB
nodepool.**

## Phase 1 solution

This solution is very similar to the os-net-config tool, and can even use it
if supported. The main idea is that the user provides a nertworking
configuration template for each logical unit (nodepool or controller nodes),
that is then rendered per node, depending on a global networks definition and a
set of states. In addition, a mapping between mac addresses and interface names
need to be provided by the user.

Three parts are required for this to work :

 * a networks definition, that is a list of networks, with the names, subnet, ip
   addresses etc.
 * a network configuration template for each of the logical units
 * a global mapping between mac addresses and interface names for each node

Example network configuration (for an os-net-config output):

```yaml
network_config:
  -
    type: ovs_bridge
    name: br-ctlplane
    addresses:
      -
        ip_netmask: {{ ip['net0']}}
    routes:
      -
        default: true
        next_hop: {{ config['net0'].gateway}}
    dns_servers: {{ config['net0'].dnsServers}}
    members:
      -
        type: ovs_bond
        name: bond1
        members:
          -
            type: interface
            name: em1
          -
            type: interface
            name: em2
  -
    type: interface
    name: em0
    addresses:
      -
        ip_netmask: {{ ip['net1']}}
```

Network configuration example

```yaml
networks:
  net0:
    netmask: 24
    addressRange:
      start: 192.168.0.10
      end: 192.168.0.250
    gateway: 192.168.0.1
    dnsServers:
      - 8.8.8.8
      - 8.8.4.4
  net1:
    netmask: 24
    addresses:
      - 192.168.1.10
      - 192.168.1.11
      - 192.168.1.12
      - 192.168.1.15
```

Some items are compulsory :

 * **netmask**
 * **addressRange** or **addresses**

The client uses either *addressRange* or *addresses* to either compute or
select an ip address. It will check if the address is available by verifying
the network states.

While generating those files for each of the node, the client would store the
current state in an object in the cluster, for example :

```yaml
kind: ConfigMap
apiVersion: v1
metadata:
  name: xxx
  namespace: xxx
data:
  net0:
    node-0: 192.168.0.10
  net1:
    node-0: 192.168.1.10
```

Whenever a node would be deleted, its entries must be removed by the client

While generating the KubeadmConfig, the client would fetch the object containing
the network states (configmap for example here) and would assign an ip address
from each of the networks that are not conflicting with already allocated ip
addresses. It would then update the configmap, before creating the
KubeadmConfig.

If two clients try to perform the same action, one will update the configmap,
then the second will get an error stipulating that the configmap has been
updated. In that case, the client should attempt the whole procedure again, to
avoid duplicated ip addresses, and take into account the newly assigned ip
address.

The output file generated by the client would then be for node-0 (using
os-net-config):

```yaml
network_config:
  -
    type: ovs_bridge
    name: br-ctlplane
    addresses:
      -
        ip_netmask: 192.0.0.10/24
    routes:
      -
        default: true
        next_hop: 192.168.0.1
    dns_servers:
      - 8.8.8.8
      - 8.8.4.4
    members:
      -
        type: ovs_bond
        name: bond1
        members:
          -
            type: interface
            name: em1
          -
            type: interface
            name: em2
  -
    type: interface
    name: em0
    addresses:
      -
        ip_netmask: 192.0.1.10/24
```

The mac mapping will be given as follows :

```yaml
node-0:
  em0: "00:c8:7c:e6:f0:2e"
  em1: "00:c8:7c:e6:f0:2f"
  em2: "00:c8:7c:e6:f0:30"
```

This will translate into an /etc/os-net-config/mapping.yaml using a script
similar to:

```sh
#!/bin/sh
eth_addr=$(/sbin/ifconfig eth0 | grep ether | awk '{print $2}')
mkdir -p /etc/os-net-config

# Create an os-net-config mapping file, note this defaults to
# /etc/os-net-config/mapping.yaml, so we use that name despite
# rendering the result as json
echo '$node_lookup' | python -c "
import json
import sys
import yaml
input = sys.stdin.readline() or '{}'
data = json.loads(input)
for node in data:
  if '${eth_addr}' in data[node].values():
    interface_mapping = {'interface_mapping': data[node]}
    with open('/etc/os-net-config/mapping.yaml', 'w') as f:
      yaml.safe_dump(interface_mapping, f, default_flow_style=False)
    break
"
```

## Phase 2

The phase 2 consists into moving the template rendering into the bootstrap
provider implementation (CABPK). The input from the user will remain unchanged,
and what is run on the node will stay the same. The logic will be extracted from
the client to be added in the bootstrap data provider.

## Considerations on the placement of the templating operation

### In the client

The issue of this method is that it does not allow to use dynamic node
management such as machine deployments or controlplanes. This prevents things
such as automated healing.

### In the bootstrap provider

This is the most versatile method. It does not have any constraint except the
complexity of the implementation. This is phase 2.

### In CAPBM

The CAPBM provider can also access all information but would then need to
modify the cloud config output of the bootstrap provider, hence making it more
complex and blurring the separation line between cloud provider and bootstrap
provider. This solution was proposed to the Metal3 community and rejected.
Stephen has presented his arguments in the
[metal3-docs PR](https://github.com/metal3-io/metal3-docs/pull/56)

### On the node

On the node it is not possible to get the network state that would be shared
across all nodes, without accessing the management cluster. The complexity would
then live in a script executed on the node. It would be more difficult to
access logs and debug information when needed.

## Considerations on the rendering of the mac mapping

### In the client

Doing it at any step before CAPBM maps the Baremetalmachine with the
Baremetalhost will require that mapping to be deterministic, as it is needed
that the baremetalhost allocated for the node is known when rendering the
mac mapping. This can be achieve by adding hostselector for each node in order
to select a specific Baremetalhost. However, this once again prevent the use of
machinedeployments or any level of dynamicity.

### In the bootstrap provider

When processing the KubeadmConfig, the selection of the BMH has not been done
yet either so the same restriction applies.

### In CAPBM

The same restriction applies, in addition of the same issue as when
rendering the network configuration template in CAPBM. This solution is
only to be exhaustive in the listing as it should not be done this way.

### On the node

This is the easiest solution, however, it requires to pass all the mac mappings
for all nodes in the cloud-init data. This could be refined per logical units if
some host selection is applied per logical unit.

## Example user configuration

```yaml
infra:
  networks:
    net0:
      netmask: 24
      addressRange:
        start: 192.168.0.10
        end: 192.168.0.250
      gateway: 192.168.0.1
      dnsServers:
        - 8.8.8.8
        - 8.8.4.4
    net1:
      netmask: 24
      addresses:
        - 192.168.1.10
        - 192.168.1.11
        - 192.168.1.12
        - 192.168.1.15
controllers:
  replicas: 1
  hostSelector: metal3.io/controller: true
  networkConfig:
    -
      type: ovs_bridge
      name: br-ctlplane
      addresses:
        -
          ip_netmask: {{ ip['net0']}}
      routes:
        -
          default: true
          next_hop: {{ config['net0'].gateway}}
      dns_servers: {{ config['net0'].dnsServers}}
      members:
        -
          type: ovs_bond
          name: bond1
          members:
            -
              type: interface
              name: em1
            -
              type: interface
              name: em2
    -
      type: interface
      name: em0
      addresses:
        -
          ip_netmask: {{ ip['net1']}}
  macMapping:
    node-0:
      em0: "00:c8:7c:e6:f0:2e"
      em1: "00:c8:7c:e6:f0:2f"
      em2: "00:c8:7c:e6:f0:30"
nodepools:
  nodepool1:
    replicas: 1
    hostSelector: metal3.io/nodepool: nodepool1
    networkConfig:
      -
        type: ovs_bridge
        name: br-ctlplane
        addresses:
          -
            ip_netmask: {{ ip['net0']}}
        routes:
          -
            default: true
            next_hop: {{ config['net0'].gateway}}
        dns_servers: {{ config['net0'].dnsServers}}
        members:
          -
            type: ovs_bond
            name: bond1
            members:
              -
                type: interface
                name: em1
              -
                type: interface
                name: em2
      -
        type: interface
        name: em0
        addresses:
          -
            ip_netmask: {{ ip['net1']}}
    macMapping:
      node-0:
        em0: "00:c8:7c:e6:f0:3e"
        em1: "00:c8:7c:e6:f0:3f"
        em2: "00:c8:7c:e6:f0:40"
```
